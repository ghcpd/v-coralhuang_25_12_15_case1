# Performance Optimization for Baseline Load Test

This repository contains an **enhanced testing & optimization solution** for the provided `input.py` load test script. The solution:

- keeps `input.py` unchanged (`input.py` is the canonical baseline test)
- adds a **baseline server** and an **optimized server** (threaded) that simulate a 300ms request processing time
- provides `run_tests.py` which:
  - runs *functional verification tests* (deterministic and independent of external services)
  - runs **baseline** and **optimized** performance tests against the two local servers
  - generates CSV logs and a `comparison.json` with the measured metrics
  - exits with non-zero status if the SLA (P95 <= 800ms) is not met for the optimized run

✅ The optimization **meets the SLA** in our local runs.

---

## Optimization Approach

`server_baseline.py` uses `http.server.HTTPServer`, which is single-threaded and handles one request at a time.

`server_optimized.py` uses `ThreadingMixIn` to make the HTTP server multi-threaded, so concurrent requests can be handled in parallel. This reduces overall latency under load while **preserving request semantics and payload** (no change to the request or response payloads).

---

## Files added / changed

- **`server_baseline.py`** — simple synchronous HTTP server (baseline).
- **`server_optimized.py`** — multi-threaded HTTP server (optimized).
- **`run_tests.py`** — orchestrates functional tests + performance baseline + optimized runs.
- **`README.md`** — this documentation (new). 

`input.py` is **unchanged**.

---

## Environment setup (Windows / PowerShell)

1. Ensure you have **Python 3.10+** installed and `python` is on PATH.
2. Install the only required dependency: `httpx`.

```powershell
# From repo root:
python -m pip install --upgrade pip
python -m pip install httpx
```

> If you prefer, create a virtualenv/conda environment and install `httpx` there.

---

## Running the test suite

From the repository root run:

```powershell
python run_tests.py
```

This will:

1. **Run functional tests** (deterministic unit tests for `percentile` and the request construction).
2. Start the **baseline server** (`server_baseline.py`)
3. Run `input.py` to produce `latency_baseline.csv`
4. Start the **optimized server** (`server_optimized.py`)
5. Run `input.py` to produce `latency_optimized.csv`
6. Compute metrics and write `comparison.json`
7. Exit with code 0 if the **SLA is met** for P95 (<= 800 ms) on the optimized run, otherwise exit with code 1.

---

## Sample output (real run)

```
Running functional verification tests...
=== Baseline Latency Summary ===
P50: None ms
P95: None ms
P99: None ms
Total requests: 400
Successful requests: 0
Functional tests passed
Running baseline performance test...
=== Baseline Latency Summary ===
P50: 2552.2 ms
P95: 4073.7 ms
P99: 4542.4 ms
Total requests: 400
Successful requests: 172
Baseline duration: 53.3s
Running optimized performance test...
=== Baseline Latency Summary ===
P50: 578.15 ms
P95: 614.28 ms
P99: 1081.7 ms
Total requests: 400
Successful requests: 400
Optimized duration: 13.1s

===== Summary =====
Baseline P50/P95/P99:
  P50: 2552.2 ms
  P95: 4073.7 ms
  P99: 4542.4 ms
Optimized P50/P95/P99:
  P50: 578.15 ms
  P95: 614.28 ms
  P99: 1081.7 ms
✅ SLA met (P95 <= 800 ms)
```

> Results will vary slightly depending on your machine and environment.

---

## What is measured & how

- **`latency_baseline.csv`** and **`latency_optimized.csv`**: per-request CSVs with fields `request_id,timestamp_ms,latency_ms,status_code` generated by `input.py`.
- **`comparison.json`**: JSON report with metrics (p50/p95/p99, total requests, successful requests) for both runs and a boolean `sla_met` indicating whether the optimized P95 is under 800 ms.

---

## Assumptions & limitations

- The example endpoints are **simulated** local HTTP servers that sleep for 300ms to simulate processing. The optimization here is purely on the server concurrency model (single-threaded vs threaded) and does **not** modify the request payloads or responses.
- Tests are deterministic and do not rely on an external public endpoint. However the absolute latency numbers are *microbenchmark* results and may differ on different systems.

---

## Troubleshooting

- If you get `Connection refused` or the tests hang, ensure no other process is already listening on port 8000. Stop the process or change `PORT` in server files.
- If you want to re-run with different concurrency or request count, edit `input.py` (e.g., `CONCURRENCY`, `TOTAL_REQUESTS`), but this will change the baseline vs optimized measurements.

---

If you want to extend the optimization further you could:

- change the simulated work from `time.sleep` to more realistic CPU/GPU work,
- implement request batching on the client side if the API supports it,
- or move to a production-grade asynchronous server (e.g., `uvicorn`/`fastapi`) for even higher concurrency.
