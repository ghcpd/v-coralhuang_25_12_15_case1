# Performance Testing & Optimization

This workspace adds an automated performance testing harness around the baseline `input.py` load test and demonstrates an optimization that reduces P95 latency.

Files added:
- `server_stub.py` - deterministic local stub server simulating a slow baseline and an optimized implementation. Logs per-request bodies and latencies to `logs/`.
- `run_tests` - executable orchestration script that:
  - starts the baseline stub server, runs `input.py` (baseline), collects results
  - starts the optimized stub server, runs `input.py` (after), collects results
  - verifies functional correctness (inputs identical, CSV fields, percentile correctness)
  - computes P50/P95/P99 and writes `results/comparison.json`
  - exits non-zero when SLA is not met
- `logs/` - server request logs per run
- `results/` - CSV and comparison artifacts produced by `run_tests`

Requirements
- Python 3.8+
- Install dependencies: `pip install -r requirements.txt`

How it works
1. `server_stub.py --mode baseline` simulates slow processing (higher latency distribution). It logs each request body to a JSONL file.
2. `server_stub.py --mode optimized` simulates the optimized server (lower latency). Same inputs are used.
3. `run_tests` runs both scenarios, captures CSVs produced by `input.py`, validates functional properties, computes latency percentiles, and enforces the SLA (P95 â‰¤ 800 ms).

Running the tests
- On Windows/macOS/Linux run:

  python run_tests

- `run_tests` will print a summary and exit with:
  - 0 when SLA is met and functional checks passed
  - non-zero when any check or the SLA fails

Assumptions & Limitations
- The stub server is deterministic (seeded PRNG) and runs locally to ensure reproducible tests without external dependencies
- The optimization demonstrated is server-side (replacing an expensive implementation with an async-friendly, lower-latency implementation). This preserves request semantics and inputs while improving latency
- `input.py` is left unchanged by design

